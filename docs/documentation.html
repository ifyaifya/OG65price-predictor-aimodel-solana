<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Price Predictor On-Chain - Documentation</title>
    <style>
        :root {
            --bg-dark: #0d1117;
            --bg-card: #161b22;
            --border: #30363d;
            --text: #c9d1d9;
            --text-muted: #8b949e;
            --accent: #58a6ff;
            --accent-green: #3fb950;
            --accent-red: #f85149;
            --accent-yellow: #d29922;
            --accent-purple: #a371f7;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, sans-serif;
            background: var(--bg-dark);
            color: var(--text);
            line-height: 1.6;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 2rem;
        }

        header {
            text-align: center;
            padding: 3rem 0;
            border-bottom: 1px solid var(--border);
            margin-bottom: 3rem;
        }

        h1 {
            font-size: 2.5rem;
            background: linear-gradient(135deg, var(--accent), var(--accent-purple));
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            margin-bottom: 1rem;
        }

        .subtitle {
            color: var(--text-muted);
            font-size: 1.2rem;
        }

        nav {
            background: var(--bg-card);
            border: 1px solid var(--border);
            border-radius: 8px;
            padding: 1.5rem;
            margin-bottom: 2rem;
            position: sticky;
            top: 1rem;
            z-index: 100;
        }

        nav h3 {
            color: var(--accent);
            margin-bottom: 1rem;
        }

        nav ul {
            list-style: none;
            display: flex;
            flex-wrap: wrap;
            gap: 1rem;
        }

        nav a {
            color: var(--text);
            text-decoration: none;
            padding: 0.5rem 1rem;
            border-radius: 4px;
            background: var(--bg-dark);
            border: 1px solid var(--border);
            transition: all 0.2s;
        }

        nav a:hover {
            border-color: var(--accent);
            color: var(--accent);
        }

        section {
            background: var(--bg-card);
            border: 1px solid var(--border);
            border-radius: 8px;
            padding: 2rem;
            margin-bottom: 2rem;
        }

        h2 {
            color: var(--accent);
            border-bottom: 2px solid var(--accent);
            padding-bottom: 0.5rem;
            margin-bottom: 1.5rem;
        }

        h3 {
            color: var(--accent-purple);
            margin: 1.5rem 0 1rem;
        }

        h4 {
            color: var(--text);
            margin: 1rem 0 0.5rem;
        }

        p {
            margin-bottom: 1rem;
        }

        code {
            background: var(--bg-dark);
            padding: 0.2rem 0.5rem;
            border-radius: 4px;
            font-family: 'Fira Code', 'Consolas', monospace;
            font-size: 0.9rem;
        }

        pre {
            background: var(--bg-dark);
            border: 1px solid var(--border);
            border-radius: 8px;
            padding: 1.5rem;
            overflow-x: auto;
            margin: 1rem 0;
        }

        pre code {
            background: none;
            padding: 0;
        }

        .diagram {
            background: #0d1117;
            border: 1px solid var(--border);
            border-radius: 8px;
            padding: 2rem;
            margin: 1.5rem 0;
            font-family: 'Fira Code', monospace;
            white-space: pre;
            overflow-x: auto;
            font-size: 0.85rem;
            line-height: 1.4;
        }

        .warning {
            background: rgba(210, 153, 34, 0.1);
            border-left: 4px solid var(--accent-yellow);
            padding: 1rem 1.5rem;
            margin: 1rem 0;
            border-radius: 0 8px 8px 0;
        }

        .success {
            background: rgba(63, 185, 80, 0.1);
            border-left: 4px solid var(--accent-green);
            padding: 1rem 1.5rem;
            margin: 1rem 0;
            border-radius: 0 8px 8px 0;
        }

        .error {
            background: rgba(248, 81, 73, 0.1);
            border-left: 4px solid var(--accent-red);
            padding: 1rem 1.5rem;
            margin: 1rem 0;
            border-radius: 0 8px 8px 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 1rem 0;
        }

        th, td {
            border: 1px solid var(--border);
            padding: 0.75rem;
            text-align: left;
        }

        th {
            background: var(--bg-dark);
            color: var(--accent);
        }

        tr:nth-child(even) {
            background: rgba(255,255,255,0.02);
        }

        .step {
            display: flex;
            gap: 1rem;
            margin: 1.5rem 0;
            align-items: flex-start;
        }

        .step-number {
            background: var(--accent);
            color: var(--bg-dark);
            width: 32px;
            height: 32px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-weight: bold;
            flex-shrink: 0;
        }

        .step-content {
            flex: 1;
        }

        .badge {
            display: inline-block;
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            font-size: 0.8rem;
            margin-right: 0.5rem;
        }

        .badge-green { background: rgba(63, 185, 80, 0.2); color: var(--accent-green); }
        .badge-yellow { background: rgba(210, 153, 34, 0.2); color: var(--accent-yellow); }
        .badge-red { background: rgba(248, 81, 73, 0.2); color: var(--accent-red); }
        .badge-blue { background: rgba(88, 166, 255, 0.2); color: var(--accent); }

        footer {
            text-align: center;
            padding: 2rem;
            color: var(--text-muted);
            border-top: 1px solid var(--border);
            margin-top: 3rem;
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>Price Predictor On-Chain</h1>
            <p class="subtitle">Neural Network on Solana with SolanaPython</p>
        </header>

        <nav>
            <h3>Table of Contents</h3>
            <ul>
                <li><a href="#overview">Overview</a></li>
                <li><a href="#ai-fundamentals">AI Fundamentals</a></li>
                <li><a href="#architecture">Architecture</a></li>
                <li><a href="#how-it-works">How It Works</a></li>
                <li><a href="#data-collection">Data Collection</a></li>
                <li><a href="#training">Training</a></li>
                <li><a href="#deployment">Deployment</a></li>
                <li><a href="#usage">Usage</a></li>
                <li><a href="#limitations">Limitations</a></li>
            </ul>
        </nav>

        <!-- OVERVIEW -->
        <section id="overview">
            <h2>1. Overview</h2>

            <p>
                The <strong>Price Predictor</strong> is a minimalist neural network
                designed to run entirely on-chain on Solana. It predicts the direction
                of the SOL/USD price over the next few minutes.
            </p>

            <h3>Key Features</h3>
            <table>
                <tr>
                    <th>Feature</th>
                    <th>Value</th>
                </tr>
                <tr>
                    <td>Architecture</td>
                    <td><code>4 inputs -> 3 hidden -> 2 outputs</code></td>
                </tr>
                <tr>
                    <td>Number of parameters</td>
                    <td>23 (15 encoder + 8 decoder)</td>
                </tr>
                <tr>
                    <td>Bytecode size</td>
                    <td>~900 bytes encoder + ~570 bytes decoder</td>
                </tr>
                <tr>
                    <td>Compute Units</td>
                    <td>~256K CU per inference (2 transactions)</td>
                </tr>
                <tr>
                    <td>Quantization</td>
                    <td>INT8 (-128 to 127)</td>
                </tr>
            </table>

            <h3>Model Outputs</h3>
            <ul style="margin-left: 2rem; margin-bottom: 1rem;">
                <li><span class="badge badge-green">+1</span> <strong>Bullish</strong> - Price expected to rise</li>
                <li><span class="badge badge-yellow">0</span> <strong>Neutral</strong> - Price stable</li>
                <li><span class="badge badge-red">-1</span> <strong>Bearish</strong> - Price expected to fall</li>
            </ul>
        </section>

        <!-- AI FUNDAMENTALS -->
        <section id="ai-fundamentals">
            <h2>2. Artificial Intelligence Fundamentals</h2>

            <p>
                This section explains in detail how AI and neural networks work,
                from basic concepts to implementation in our model.
            </p>

            <h3>2.1 What is Machine Learning?</h3>

            <p>
                <strong>Machine Learning</strong> is a branch of AI that enables computers
                to <em>learn from data</em> rather than being explicitly programmed.
            </p>

            <div class="diagram">
TRADITIONAL PROGRAMMING:
+----------+     +----------+     +----------+
|  Rules   | --> | Program  | --> | Result   |
| (human)  |     |          |     |          |
+----------+     +----------+     +----------+

MACHINE LEARNING:
+----------+     +----------+     +----------+
|  Data    | --> |    ML    | --> |  Rules   |
| + Labels |     | Algorithm|     | (model)  |
+----------+     +----------+     +----------+
            </div>

            <p>
                <strong>Concrete example:</strong> To detect if an email is spam:
            </p>
            <ul style="margin-left: 2rem; margin-bottom: 1rem;">
                <li><strong>Traditional approach:</strong> A human writes rules ("if contains 'free', it's spam")</li>
                <li><strong>ML approach:</strong> Give 10,000 emails (spam and not spam) to the algorithm, it finds the patterns itself</li>
            </ul>

            <h3>2.2 Types of Learning</h3>

            <table>
                <tr>
                    <th>Type</th>
                    <th>Description</th>
                    <th>Example</th>
                </tr>
                <tr>
                    <td><strong>Supervised</strong></td>
                    <td>Data provided WITH correct answers (labels)</td>
                    <td>Our model: price -> direction (up/down/neutral)</td>
                </tr>
                <tr>
                    <td>Unsupervised</td>
                    <td>Data provided WITHOUT labels, algorithm finds patterns</td>
                    <td>Clustering similar customers</td>
                </tr>
                <tr>
                    <td>Reinforcement</td>
                    <td>Agent learns through trial and error with rewards</td>
                    <td>AlphaGo, robots</td>
                </tr>
            </table>

            <div class="success">
                <strong>Our model</strong> uses <strong>supervised learning</strong>:
                we give it examples of past prices with the actual future price direction,
                and it learns to predict.
            </div>

            <h3>2.3 What is an Artificial Neuron?</h3>

            <p>
                An <strong>artificial neuron</strong> is the basic unit of a neural network.
                It's inspired (in simplified form) by the biological neuron:
            </p>

            <div class="diagram">
BIOLOGICAL NEURON:                    ARTIFICIAL NEURON:

   Dendrites                              Inputs (x1, x2, x3)
      |                                         |
      v                                         v
  +-------+                               +----------+
  | Cell  | --> Axon --> Synapses        | Weighted | --> Activation --> Output
  | Body  |                               |   Sum    |
  +-------+                               +----------+

  Receives                                y = activation(w1*x1 + w2*x2 + w3*x3 + b)
  signals,
  integrates,                             w = weights (importance of each input)
  and fires                               b = bias (activation threshold)
            </div>

            <h3>2.4 How Does a Neuron Work?</h3>

            <div class="step">
                <div class="step-number">1</div>
                <div class="step-content">
                    <strong>Weighted Inputs</strong>
                    <p>
                        Each input <code>x</code> is multiplied by a <strong>weight</strong> <code>w</code>.
                        Weights represent the importance of each input.
                    </p>
                    <pre><code>weighted_input = x1*w1 + x2*w2 + x3*w3</code></pre>
                </div>
            </div>

            <div class="step">
                <div class="step-number">2</div>
                <div class="step-content">
                    <strong>Adding Bias</strong>
                    <p>
                        The <strong>bias</strong> <code>b</code> is added to shift the activation threshold.
                        It's like a "sensitivity threshold" for the neuron.
                    </p>
                    <pre><code>z = x1*w1 + x2*w2 + x3*w3 + b</code></pre>
                </div>
            </div>

            <div class="step">
                <div class="step-number">3</div>
                <div class="step-content">
                    <strong>Activation Function</strong>
                    <p>
                        A non-linear function is applied to introduce complexity.
                        Without it, the network could only model linear relationships.
                    </p>
                    <pre><code>output = activation(z)</code></pre>
                </div>
            </div>

            <h3>2.4.1 Understanding Weights: A Concrete Example</h3>

            <p>
                Let's understand weights with a simple real-world analogy and then apply it to our model.
            </p>

            <div class="warning">
                <strong>Analogy: Hiring Decision</strong>
                <p>
                    Imagine you're hiring someone. You evaluate 3 criteria: Experience, Education, Interview.
                    But they don't all matter equally to you:
                </p>
                <ul style="margin-top: 0.5rem;">
                    <li>Experience: Very important (weight = 0.5)</li>
                    <li>Education: Somewhat important (weight = 0.3)</li>
                    <li>Interview: Less important (weight = 0.2)</li>
                </ul>
                <p style="margin-top: 0.5rem;">
                    Final score = 0.5 * Experience + 0.3 * Education + 0.2 * Interview
                </p>
            </div>

            <p><strong>In neural networks, weights work the same way:</strong></p>

            <div class="diagram">
WEIGHT = How much attention the neuron pays to each input

+------------------+--------------------------------------------------+
| Weight Value     | Meaning                                          |
+------------------+--------------------------------------------------+
| Large positive   | "This input strongly INCREASES my output"       |
| (e.g., +50)      | When input goes up, output goes up a lot        |
+------------------+--------------------------------------------------+
| Small positive   | "This input slightly increases my output"       |
| (e.g., +5)       | When input goes up, output goes up a little     |
+------------------+--------------------------------------------------+
| Zero             | "I ignore this input completely"                |
| (0)              | Input has no effect on output                   |
+------------------+--------------------------------------------------+
| Small negative   | "This input slightly decreases my output"       |
| (e.g., -5)       | When input goes up, output goes DOWN a little   |
+------------------+--------------------------------------------------+
| Large negative   | "This input strongly DECREASES my output"       |
| (e.g., -50)      | When input goes up, output goes DOWN a lot      |
+------------------+--------------------------------------------------+
            </div>

            <h3>2.4.2 Our Model's Weights Explained</h3>

            <p>
                Our model has <strong>23 weights total</strong>, organized in two layers:
            </p>

            <div class="diagram">
LAYER 1: INPUT -> HIDDEN (15 weights)
==========================================

We have 4 inputs and 3 hidden neurons.
Each hidden neuron connects to ALL 4 inputs = 4 weights per neuron
Plus 1 bias per neuron

       I0  I1  I2  I3   bias
       |   |   |   |    |
H0:   w0  w3  w6  w9   b0    (5 values)
H1:   w1  w4  w7  w10  b1    (5 values)
H2:   w2  w5  w8  w11  b2    (5 values)
                              -----------
                              15 total

LAYER 2: HIDDEN -> OUTPUT (8 weights)
==========================================

We have 3 hidden neurons and 2 outputs.
Each output connects to ALL 3 hidden neurons = 3 weights per output
Plus 1 bias per output

       H0   H1   H2   bias
       |    |    |    |
O0:   w0   w2   w4   b0    (4 values) = Direction
O1:   w1   w3   w5   b1    (4 values) = Confidence
                            -----------
                            8 total

TOTAL: 15 + 8 = 23 weights
            </div>

            <h3>2.4.3 Step-by-Step Calculation Example</h3>

            <p>Let's trace through a real calculation with our trained weights:</p>

            <pre><code># Our actual trained weights (from weights_debug.txt):
# W1 (input->hidden): 12 weights organized as 4x3 matrix
# b1 (hidden biases): 3 values

W1 = [23, -12, 1,      # weights for input 0 -> h0, h1, h2
      -41, -10, 0,     # weights for input 1 -> h0, h1, h2
      -127, -12, 1,    # weights for input 2 -> h0, h1, h2
      76, -21, 1]      # weights for input 3 -> h0, h1, h2

b1 = [61, 127, 123]    # biases for h0, h1, h2</code></pre>

            <p><strong>Example: Bullish market signal</strong></p>
            <pre><code># Input features (normalized 0-255):
I = [160,   # vwap_ratio: 160 = price above VWAP (bullish signal)
     180,   # volume_accel: 180 = increasing volume
     155,   # orderbook_imbal: 155 = more buyers than sellers
     175]   # momentum: 175 = upward trend

# Calculate hidden neuron h0:
h0 = b1[0] + I[0]*W1[0] + I[1]*W1[3] + I[2]*W1[6] + I[3]*W1[9]
h0 = 61 + 160*23 + 180*(-41) + 155*(-127) + 175*76
h0 = 61 + 3680 - 7380 - 19685 + 13300
h0 = -10024

# Apply ReLU: max(0, -10024) = 0
h0 = 0  # This neuron is "off" for this input

# Calculate hidden neuron h1:
h1 = b1[1] + I[0]*W1[1] + I[1]*W1[4] + I[2]*W1[7] + I[3]*W1[10]
h1 = 127 + 160*(-12) + 180*(-10) + 155*(-12) + 175*(-21)
h1 = 127 - 1920 - 1800 - 1860 - 3675
h1 = -9128

# Apply ReLU: max(0, -9128) = 0
h1 = 0  # This neuron is also "off"

# Calculate hidden neuron h2:
h2 = b2[2] + I[0]*W1[2] + I[1]*W1[5] + I[2]*W1[8] + I[3]*W1[11]
h2 = 123 + 160*1 + 180*0 + 155*1 + 175*1
h2 = 123 + 160 + 0 + 155 + 175
h2 = 613

# Apply ReLU: max(0, 613) = 613
h2 = 613  # This neuron is "on" and active!</code></pre>

            <p><strong>What do these weights tell us?</strong></p>

            <div class="success">
                <strong>Interpreting the weights:</strong>
                <ul style="margin-top: 0.5rem;">
                    <li><strong>Neuron h2</strong> has small positive weights (1, 0, 1, 1) - it's a "sum detector" that activates when all inputs are high</li>
                    <li><strong>Neuron h0</strong> has mixed weights (+23, -41, -127, +76) - it's looking for a specific pattern (high I0, low I1/I2, high I3)</li>
                    <li><strong>Neuron h1</strong> has all negative weights - it's an "inhibitor" that fires when inputs are LOW</li>
                </ul>
            </div>

            <h3>2.4.4 Why These Specific Weight Values?</h3>

            <p>
                The weights are <strong>learned during training</strong>, not set by humans.
                The training process automatically finds values that minimize prediction errors:
            </p>

            <div class="diagram">
TRAINING PROCESS:

1. START: Random weights
   W = [0.1, -0.3, 0.5, ...]  (random values)

2. PREDICT: Use current weights to make predictions
   prediction = forward_pass(inputs, W)

3. MEASURE ERROR: Compare to actual result
   error = (prediction - actual)^2

4. ADJUST: Slightly modify weights to reduce error
   W_new = W_old - learning_rate * gradient

5. REPEAT: Steps 2-4 thousands of times

6. RESULT: Weights that produce good predictions
   W = [23, -12, 1, -41, ...]  (learned values)

The final weights encode "knowledge" the model learned from data!
            </div>

            <h3>2.4.5 Weight Storage in Our Model</h3>

            <p>
                Weights are stored as <strong>INT8</strong> values (-128 to 127) to save space:
            </p>

            <div class="diagram">
MEMORY LAYOUT (Encoder - 19 bytes):

Byte:  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18
      +---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+
      |W1 weights (12 bytes)              |b1 (3)|Input features (4)|
      +---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+
      |23 |-12| 1 |-41|-10| 0 |-127|-12| 1|76 |-21| 1 |61 |127|123| I0| I1| I2| I3|

MEMORY LAYOUT (Decoder - 8 bytes):

Byte:  0   1   2   3   4   5   6   7
      +---+---+---+---+---+---+---+---+
      |W2 weights (6 bytes)  |b2 (2)|
      +---+---+---+---+---+---+---+---+
      |-1 | 0 |-49| 6 |50 |127|-7 |127|

Total: 19 + 8 = 27 bytes for all model parameters!
            </div>

            <h3>2.4.6 Visual Summary: The Complete Weight Flow</h3>

            <div class="diagram">
                     INPUTS (4 bytes)              WEIGHTS (15 params)           HIDDEN (3 neurons)
                    +----+----+----+----+
                    |I0  |I1  |I2  |I3  |
                    |160 |180 |155 |175 |
                    +----+----+----+----+
                       |    |    |    |
                       |    |    |    |         W1[0]=23
                       +----+----+----+---------*--------> h0 = ReLU(61 + 160*23 + ...)
                       |    |    |    |         W1[3]=-41           |
                       |    |    |    |         W1[6]=-127          |
                       |    |    |    |         W1[9]=76            |
                       |    |    |    |         b1[0]=61            |
                       |    |    |    |                             |
                       |    |    |    |         W1[1]=-12           |     WEIGHTS (8 params)
                       +----+----+----+---------*--------> h1 = 0   +----*-----> O0 (Direction)
                       |    |    |    |         ...                 |    W2, b2
                       |    |    |    |                             |
                       |    |    |    |         W1[2]=1             |
                       +----+----+----+---------*--------> h2 = 613 +----*-----> O1 (Confidence)
                                                ...

    EACH ARROW (*) represents a multiplication by a weight!
    The model learns which weights to use to make accurate predictions.
            </div>

            <h3>2.5 Activation Functions</h3>

            <p>
                Activation functions introduce <strong>non-linearity</strong>,
                allowing the network to learn complex patterns.
            </p>

            <div class="diagram">
RELU (Rectified Linear Unit):          SIGMOID:

     y                                      y
     |      /                              1|    ------
     |     /                               |   /
   0 +----+                              0.5|  |
     |    0    x                           0+--------
     |                                      0    x

   ReLU(x) = max(0, x)                 Sigmoid(x) = 1/(1+e^-x)
   - Simple and fast                   - Output between 0 and 1
   - Our model uses this               - Good for probabilities
            </div>

            <p>
                <strong>Our model</strong> uses <strong>ReLU</strong> because:
            </p>
            <ul style="margin-left: 2rem; margin-bottom: 1rem;">
                <li>Very simple computation: <code>max(0, x)</code></li>
                <li>No gradient problems for large values</li>
                <li>Perfect for integer computations (INT8)</li>
            </ul>

            <h3>2.6 What is a Neural Network?</h3>

            <p>
                A <strong>neural network</strong> is a collection of neurons organized in <strong>layers</strong>:
            </p>

            <div class="diagram">
INPUT LAYER              HIDDEN LAYER             OUTPUT LAYER

    [x0] --------\
                  +----> [h0] ----\
    [x1] --------+                 +----> [y0]
                  +----> [h1] ----+
    [x2] --------+                 +----> [y1]
                  +----> [h2] ----/
    [x3] --------/

  4 neurons               3 neurons               2 neurons
  (our 4 features)        (hidden layer)          (direction, confidence)

EACH ARROW = a weight (w)
EACH NEURON = bias (b) + activation
            </div>

            <p>
                <strong>Why hidden layers?</strong> They allow the network to learn
                <strong>intermediate representations</strong> of the data. For example:
            </p>
            <ul style="margin-left: 2rem; margin-bottom: 1rem;">
                <li>Input: raw features (price, volume, etc.)</li>
                <li>Hidden layer: abstract concepts ("strong trend", "volatile market")</li>
                <li>Output: final decision (up/down/neutral)</li>
            </ul>

            <h3>2.7 Forward Propagation</h3>

            <p>
                <strong>Forward propagation</strong> is the process of calculating
                the output from inputs, layer by layer:
            </p>

            <pre><code># Our model 4->3->2

# Inputs (normalized features 0-255)
I = [vwap_ratio, volume_accel, orderbook_imbal, momentum]

# === HIDDEN LAYER (3 neurons) ===

# Neuron h0
z0 = b1[0] + I[0]*W1[0,0] + I[1]*W1[1,0] + I[2]*W1[2,0] + I[3]*W1[3,0]
h0 = ReLU(z0) = max(0, z0)

# Neuron h1
z1 = b1[1] + I[0]*W1[0,1] + I[1]*W1[1,1] + I[2]*W1[2,1] + I[3]*W1[3,1]
h1 = ReLU(z1) = max(0, z1)

# Neuron h2
z2 = b1[2] + I[0]*W1[0,2] + I[1]*W1[1,2] + I[2]*W1[2,2] + I[3]*W1[3,2]
h2 = ReLU(z2) = max(0, z2)

# === OUTPUT LAYER (2 neurons) ===

# Direction score
o0 = b2[0] + h0*W2[0,0] + h1*W2[1,0] + h2*W2[2,0]

# Confidence score
o1 = b2[1] + h0*W2[0,1] + h1*W2[1,1] + h2*W2[2,1]

# Interpretation
direction = "UP" if o0 > 0 else "DOWN"</code></pre>

            <h3>2.8 Loss Function</h3>

            <p>
                The <strong>loss function</strong> measures how far the model's predictions
                are from reality. The lower the loss, the better.
            </p>

            <div class="diagram">
MEAN SQUARED ERROR (MSE):

            n
Loss = (1/n) * SUM (prediction_i - actual_i)^2
           i=1

Example:
- Prediction: direction = +30 (bullish)
- Actual: direction = +50 (very bullish)
- Error: (30 - 50)^2 = 400

Goal: minimize this error!
            </div>

            <p>
                <strong>Our model</strong> uses MSE (Mean Squared Error) plus
                <strong>L2 regularization</strong> to prevent overfitting:
            </p>

            <pre><code>Loss = MSE(predictions, actual) + lambda * sum(weights^2)
#      Prediction error          + Penalty for large weights</code></pre>

            <h3>2.9 Backpropagation and Gradient Descent</h3>

            <p>
                <strong>Learning</strong> happens by adjusting weights to minimize the loss.
                This uses two key concepts:
            </p>

            <h4>Gradient Descent</h4>

            <div class="diagram">
Imagine a mountain (the loss function):

        Loss
          ^
          |    X <- We start here (random weights)
          |   /|
          |  / |
          | /  |
          |/   v
          +----*-----> Weights
               ^ Minimum (best weights)

At each step:
1. Calculate the slope (gradient)
2. Take a small step in the descending direction
3. Repeat until minimum
            </div>

            <p>
                The weight update formula:
            </p>
            <pre><code>new_weight = old_weight - learning_rate * gradient

# learning_rate = step size (e.g., 0.001)
# gradient = direction of steepest slope</code></pre>

            <h4>Backpropagation</h4>

            <p>
                <strong>Backpropagation</strong> is the algorithm to efficiently calculate
                the gradient of each weight. It propagates the error from output to input:
            </p>

            <div class="diagram">
FORWARD:  Inputs -----> Hidden layer -----> Output -----> Loss
                                                            |
BACKWARD: dW1 <-------- dW2 <-------------- dLoss <---------+

At each layer, we calculate:
1. How much this layer contributed to the final error
2. How to adjust its weights to reduce this error
            </div>

            <h3>2.10 Adam Optimizer</h3>

            <p>
                <strong>Adam</strong> (Adaptive Moment Estimation) is an advanced optimizer
                that improves basic gradient descent:
            </p>

            <table>
                <tr>
                    <th>Feature</th>
                    <th>Gradient Descent</th>
                    <th>Adam</th>
                </tr>
                <tr>
                    <td>Learning rate</td>
                    <td>Fixed</td>
                    <td>Adaptive per weight</td>
                </tr>
                <tr>
                    <td>Momentum</td>
                    <td>No</td>
                    <td>Yes (avoids oscillations)</td>
                </tr>
                <tr>
                    <td>Convergence</td>
                    <td>Slow</td>
                    <td>Fast</td>
                </tr>
            </table>

            <pre><code># Adam parameters (our model)
learning_rate = 0.001
beta1 = 0.9   # Momentum for gradient
beta2 = 0.999 # Momentum for gradient^2</code></pre>

            <h3>2.11 Overfitting and Regularization</h3>

            <p>
                <strong>Overfitting</strong> occurs when the model "memorizes" the training data
                instead of learning general patterns:
            </p>

            <div class="diagram">
UNDERFITTING            GOOD FIT               OVERFITTING
(too simple)            (generalizes)          (memorizes)

    .  .                    .  .                   .  .
   .    .                  .--.                   .~~.
  .      .                .    .                 .    .
 .        .              .      .               .      .
.          .            .        .             .        .

- High error            - Low error            - VERY low error
  on train+test           on train+test          on train
                                               - High error on test
            </div>

            <p>
                To prevent overfitting, we use:
            </p>

            <ul style="margin-left: 2rem; margin-bottom: 1rem;">
                <li><strong>L2 Regularization:</strong> Penalizes large weights</li>
                <li><strong>Early stopping:</strong> Stops training when validation stops improving</li>
                <li><strong>Data augmentation:</strong> Adds noise to data</li>
            </ul>

            <h3>2.12 INT8 Quantization</h3>

            <p>
                <strong>Quantization</strong> reduces the precision of weights to save
                memory and speed up computations:
            </p>

            <div class="diagram">
FLOAT32 (normal):                 INT8 (quantized):

32 bits per weight                8 bits per weight
-3.4e38 to +3.4e38                -128 to +127
Precision: ~7 digits              Precision: ~2 digits

Example:
Original weight: 0.7854201        Quantized weight: 100
                                  (0.7854201 * 127 = 99.75 -> 100)

Advantages:
- 4x less memory (32/8)
- Faster computations (integers vs floats)
- Compatible with blockchain constraints

Disadvantages:
- Loss of precision (~2-5%)
            </div>

            <pre><code># Quantization code
def quantize(weight, bits=8):
    max_val = max(abs(weight.min()), abs(weight.max()))
    scale = (2**(bits-1) - 1) / max_val  # 127 for INT8
    return round(weight * scale)

# Example
float_weight = 0.7854201
int8_weight = round(0.7854201 * 127) = 100</code></pre>

            <h3>2.13 Summary: Complete Training Cycle</h3>

            <div class="diagram">
+-------------------------------------------------------------------+
|                       TRAINING CYCLE                              |
+-------------------------------------------------------------------+
|                                                                   |
|  1. INITIALIZATION                                                |
|     - Random weights                                              |
|     - Training data prepared                                      |
|                                                                   |
|  2. TRAINING LOOP (repeat N epochs):                              |
|     +----------------------------------------------------+        |
|     | For each example (features, label):                |        |
|     |   a. Forward: calculate prediction                 |        |
|     |   b. Loss: measure error                           |        |
|     |   c. Backward: calculate gradients                 |        |
|     |   d. Update: adjust weights (Adam)                 |        |
|     +----------------------------------------------------+        |
|     |                                                    |        |
|     | Validate on validation data                        |        |
|     | If no improvement: early stopping                  |        |
|     +----------------------------------------------------+        |
|                                                                   |
|  3. EXPORT                                                        |
|     - Quantize weights to INT8                                    |
|     - Save for deployment                                         |
|                                                                   |
+-------------------------------------------------------------------+
            </div>

            <div class="success">
                <strong>Key takeaways:</strong>
                <ul style="margin-top: 0.5rem;">
                    <li>A neuron = weighted sum + bias + activation</li>
                    <li>Forward = calculate output from inputs</li>
                    <li>Loss = measure of error</li>
                    <li>Backprop = calculate how to adjust each weight</li>
                    <li>Gradient descent = adjust weights to minimize error</li>
                    <li>Adam = improved version of gradient descent</li>
                    <li>Regularization = prevent overfitting</li>
                    <li>Quantization = reduce precision for deployment</li>
                </ul>
            </div>
        </section>

        <!-- ARCHITECTURE -->
        <section id="architecture">
            <h2>3. Network Architecture</h2>

            <p>
                The model uses a simple feedforward architecture with one hidden layer
                and ReLU activation function.
            </p>

            <div class="diagram">
+----------+     +------------+     +-----------+
| INPUTS   |     | HIDDEN     |     | OUTPUTS   |
| (4)      |     | (3)        |     | (2)       |
+----------+     +------------+     +-----------+

  [I0] --------\
               +----> [H0] ----\
  [I1] --------+               +----> [O0] Direction
               +----> [H1] ----+
  [I2] --------+               +----> [O1] Confidence
               +----> [H2] ----/
  [I3] --------/

+----------+     +------------+     +-----------+
| Features |     | ReLU       |     | Linear    |
| 0-255    |     | max(0,x)   |     | score     |
+----------+     +------------+     +-----------+
            </div>

            <h3>Splitting into 2 Transactions</h3>

            <p>
                Due to the 946 bytes per transaction limit on SolanaPython,
                the model is split into two parts:
            </p>

            <div class="diagram">
TX1: ENCODER                         TX2: DECODER
+---------------------------+        +---------------------------+
| Inputs: [I0, I1, I2, I3]  |        | Reads hidden state        |
| W1: 12 weights (4x3)      |        | W2: 6 weights (3x2)       |
| b1: 3 biases              |        | b2: 2 biases              |
|                           |        |                           |
| h = W1 * I + b1           |        | out = W2 * h + b2         |
| h = ReLU(h)               |        |                           |
|                           |        | Direction = sign(out[0])  |
| Writes: [h0, h1, h2]      |        | Confidence = out[1]       |
+---------------------------+        +---------------------------+
            |                                    |
            v                                    v
     [Solana Account]                     [Final Result]
     Hidden State (3 bytes)
            </div>

            <h3>Weight Details</h3>
            <table>
                <tr>
                    <th>Layer</th>
                    <th>Shape</th>
                    <th>Parameters</th>
                    <th>Description</th>
                </tr>
                <tr>
                    <td>W1</td>
                    <td>4 x 3</td>
                    <td>12</td>
                    <td>Input -> hidden weights</td>
                </tr>
                <tr>
                    <td>b1</td>
                    <td>3</td>
                    <td>3</td>
                    <td>Hidden layer biases</td>
                </tr>
                <tr>
                    <td>W2</td>
                    <td>3 x 2</td>
                    <td>6</td>
                    <td>Hidden -> output weights</td>
                </tr>
                <tr>
                    <td>b2</td>
                    <td>2</td>
                    <td>2</td>
                    <td>Output layer biases</td>
                </tr>
                <tr>
                    <td><strong>Total</strong></td>
                    <td>-</td>
                    <td><strong>23</strong></td>
                    <td>-</td>
                </tr>
            </table>
        </section>

        <!-- HOW IT WORKS -->
        <section id="how-it-works">
            <h2>4. How It Works</h2>

            <h3>4.1 The 4 Input Features</h3>

            <p>The model receives 4 normalized features (0-255):</p>

            <table>
                <tr>
                    <th>Feature</th>
                    <th>Description</th>
                    <th>Interpretation</th>
                </tr>
                <tr>
                    <td><code>vwap_ratio</code></td>
                    <td>Price / VWAP (Volume Weighted Avg Price)</td>
                    <td>&gt;128 = above VWAP (bullish)</td>
                </tr>
                <tr>
                    <td><code>volume_accel</code></td>
                    <td>Volume acceleration</td>
                    <td>&gt;128 = increasing volume</td>
                </tr>
                <tr>
                    <td><code>orderbook_imbal</code></td>
                    <td>Order book imbalance</td>
                    <td>&gt;128 = more buyers</td>
                </tr>
                <tr>
                    <td><code>momentum</code></td>
                    <td>Short-term momentum</td>
                    <td>&gt;128 = upward trend</td>
                </tr>
            </table>

            <h3>4.2 Forward Pass Calculation</h3>

            <p><strong>Step 1: Encoder (TX1)</strong></p>
            <pre><code># W1 weights reorganized: [w00,w10,w20,w30, w01,w11,w21,w31, w02,w12,w22,w32]
W = [23,-12,1,-41,-10,0,-127,-12,1,76,-21,1,61,127,123]
I = [vwap_ratio, volume_accel, orderbook_imbal, momentum]

# Calculate hidden layer
h0 = W[12] + I[0]*W[0] + I[1]*W[3] + I[2]*W[6] + I[3]*W[9]
h1 = W[13] + I[0]*W[1] + I[1]*W[4] + I[2]*W[7] + I[3]*W[10]
h2 = W[14] + I[0]*W[2] + I[1]*W[5] + I[2]*W[8] + I[3]*W[11]

# ReLU
h0 = max(0, h0)
h1 = max(0, h1)
h2 = max(0, h2)

# Write to Solana account
write_to_account([h0 % 256, h1 % 256, h2 % 256])</code></pre>

            <p><strong>Step 2: Decoder (TX2)</strong></p>
            <pre><code># W2 weights: [w00,w01, w10,w11, w20,w21] + biases [b0, b1]
W = [-1, 0, -49, 6, 50, 127, -7, 127]

# Read hidden state from account
h = read_from_account()  # [h0, h1, h2]

# Calculate output
o0 = W[6] + h[0]*W[0] + h[1]*W[2] + h[2]*W[4]  # Direction
o1 = W[7] + h[0]*W[1] + h[1]*W[3] + h[2]*W[5]  # Confidence

# Interpretation
direction = "BULLISH" if o0 > 10 else ("BEARISH" if o0 < -10 else "NEUTRAL")
confidence = o1</code></pre>

            <h3>4.3 INT8 Quantization</h3>

            <p>
                Weights are quantized from float32 to INT8 to reduce size
                and speed up calculations:
            </p>

            <pre><code># Quantization
max_val = max(abs(weights.min()), abs(weights.max()))
scale = 127 / max_val
quantized = round(weights * scale)  # -128 to 127</code></pre>

            <div class="warning">
                <strong>Note:</strong> INT8 quantization introduces precision loss.
                Weights are stored on 8 bits instead of 32, which can affect accuracy.
            </div>
        </section>

        <!-- DATA COLLECTION -->
        <section id="data-collection">
            <h2>5. Data Collection</h2>

            <h3>5.1 Data Sources</h3>

            <p>The <code>collect_data.js</code> script collects data from:</p>
            <ul style="margin-left: 2rem; margin-bottom: 1rem;">
                <li><strong>Pyth Network</strong> - Real-time SOL/USD prices</li>
                <li><strong>Binance API</strong> - Historical data (1-minute klines)</li>
            </ul>

            <h3>5.2 Usage</h3>

            <div class="step">
                <div class="step-number">1</div>
                <div class="step-content">
                    <strong>Start real-time collection</strong>
                    <pre><code>node src/scripts/collect_data.js --duration 3600 --interval 5</code></pre>
                    <p>Options:</p>
                    <ul style="margin-left: 1.5rem;">
                        <li><code>--duration</code>: Duration in seconds (default: 3600)</li>
                        <li><code>--interval</code>: Sampling interval (default: 5s)</li>
                        <li><code>--output</code>: Output file</li>
                    </ul>
                </div>
            </div>

            <div class="step">
                <div class="step-number">2</div>
                <div class="step-content">
                    <strong>Or use historical Binance data</strong>
                    <pre><code># Download 5000 1-minute klines
curl "https://api.binance.com/api/v3/klines?symbol=SOLUSDT&interval=1m&limit=1000" > data.json</code></pre>
                </div>
            </div>

            <h3>5.3 Data Format</h3>

            <pre><code># CSV: data/sol_market_data.csv
timestamp,price,vwap_ratio,volume_accel,orderbook_imbal,volatility,liquidity,momentum,direction
1766022960000,123.37,118,129,126,153,49,122,1
1766023020000,123.37,116,133,123,127,19,122,1
...</code></pre>

            <table>
                <tr>
                    <th>Column</th>
                    <th>Type</th>
                    <th>Description</th>
                </tr>
                <tr>
                    <td>timestamp</td>
                    <td>int</td>
                    <td>UNIX timestamp in ms</td>
                </tr>
                <tr>
                    <td>price</td>
                    <td>float</td>
                    <td>SOL/USD price</td>
                </tr>
                <tr>
                    <td>vwap_ratio - momentum</td>
                    <td>int (0-255)</td>
                    <td>Normalized features</td>
                </tr>
                <tr>
                    <td>direction</td>
                    <td>int (-1, 0, 1)</td>
                    <td>Label: down/stable/up</td>
                </tr>
            </table>
        </section>

        <!-- TRAINING -->
        <section id="training">
            <h2>6. Training the Model</h2>

            <h3>6.1 Training Pipeline</h3>

            <div class="step">
                <div class="step-number">1</div>
                <div class="step-content">
                    <strong>Prepare data</strong>
                    <p>Ensure <code>data/sol_market_data.csv</code> contains at least 1000 samples.</p>
                </div>
            </div>

            <div class="step">
                <div class="step-number">2</div>
                <div class="step-content">
                    <strong>Start training</strong>
                    <pre><code>python3 src/scripts/train_v2.py --epochs 300 --hidden 3</code></pre>
                    <p>Options:</p>
                    <ul style="margin-left: 1.5rem;">
                        <li><code>--data</code>: Path to CSV (default: data/sol_market_data.csv)</li>
                        <li><code>--epochs</code>: Number of epochs (default: 200)</li>
                        <li><code>--hidden</code>: Hidden layer size (default: 3)</li>
                        <li><code>--output</code>: Weights output folder (default: weights/)</li>
                        <li><code>--cv</code>: Enable cross-validation</li>
                    </ul>
                </div>
            </div>

            <div class="step">
                <div class="step-number">3</div>
                <div class="step-content">
                    <strong>Verify results</strong>
                    <pre><code>cat weights/weights_debug.txt</code></pre>
                </div>
            </div>

            <h3>6.2 Training Algorithm</h3>

            <table>
                <tr>
                    <th>Component</th>
                    <th>Implementation</th>
                </tr>
                <tr>
                    <td>Optimizer</td>
                    <td>Adam (lr=0.001, beta1=0.9, beta2=0.999)</td>
                </tr>
                <tr>
                    <td>Loss function</td>
                    <td>MSE + L2 regularization</td>
                </tr>
                <tr>
                    <td>Regularization</td>
                    <td>L2 (weight_decay=0.001)</td>
                </tr>
                <tr>
                    <td>Early stopping</td>
                    <td>Patience = 20 epochs</td>
                </tr>
                <tr>
                    <td>Class balancing</td>
                    <td>Upsampling minority classes</td>
                </tr>
            </table>

            <h3>6.3 Output Files</h3>

            <pre><code>weights/
  encoder_4_3.bin    # Encoder weights (19 bytes)
  decoder_4_3.bin    # Decoder weights (8 bytes)
  weights_debug.txt  # Human-readable weights for debugging</code></pre>
        </section>

        <!-- DEPLOYMENT -->
        <section id="deployment">
            <h2>7. Deployment on Solana</h2>

            <h3>7.1 Prerequisites</h3>

            <ul style="margin-left: 2rem; margin-bottom: 1rem;">
                <li>Solana CLI installed and configured for devnet</li>
                <li>Wallet with at least 0.5 SOL</li>
                <li>SolanaPython compiler (<code>pika_compile</code>)</li>
            </ul>

            <div class="step">
                <div class="step-number">1</div>
                <div class="step-content">
                    <strong>Configure Solana</strong>
                    <pre><code>solana config set --url devnet
solana airdrop 1  # If you need SOL</code></pre>
                </div>
            </div>

            <div class="step">
                <div class="step-number">2</div>
                <div class="step-content">
                    <strong>Compile Python scripts</strong>
                    <pre><code># The pika_compile compiler converts Python to bytecode
./tools/pika_compile -f src/python/price_4_3_s1_trained.py -o encoder.bin
./tools/pika_compile -f src/python/price_4_3_s2_trained.py -o decoder.bin</code></pre>
                </div>
            </div>

            <div class="step">
                <div class="step-number">3</div>
                <div class="step-content">
                    <strong>Run E2E tests</strong>
                    <pre><code>node src/scripts/test.js</code></pre>
                    <p>Expected output:</p>
                    <pre><code>=================================================
Price Predictor E2E Tests (4->3->2)
=================================================
Passed: 4
Failed: 0

All tests passed!</code></pre>
                </div>
            </div>

            <h3>7.2 SolanaPython Program</h3>

            <p>The model runs on the deployed SolanaPython program:</p>

            <pre><code>Program ID: AdvFUgScZPQmnkhCZ1ZFMN7c1rsanoE7TfYbikggUAxM

Execution modes:
- MODE_EXECUTE_SCRIPT (0x00): Execute Python source code
- MODE_EXECUTE_BYTECODE (0x02): Execute compiled bytecode</code></pre>

            <div class="success">
                <strong>Recommended mode:</strong> Use <code>MODE_EXECUTE_BYTECODE</code> (0x02)
                for better performance and reduced transaction size.
            </div>
        </section>

        <!-- USAGE -->
        <section id="usage">
            <h2>8. Usage</h2>

            <h3>8.1 On-chain Inference</h3>

            <pre><code>// JavaScript with @solana/web3.js

const { Connection, Transaction, TransactionInstruction } = require("@solana/web3.js");

async function predict(features) {
  // 1. Generate encoder bytecode with features
  const encoderBytecode = generateEncoderBytecode(features);

  // 2. Create account for hidden state
  const hiddenAccount = await createAccount(3); // 3 bytes

  // 3. TX1: Encoder
  const encTx = new Transaction();
  encTx.add(ComputeBudgetProgram.setComputeUnitLimit({ units: 200_000 }));
  encTx.add(new TransactionInstruction({
    keys: [
      { pubkey: payer.publicKey, isSigner: true, isWritable: true },
      { pubkey: hiddenAccount, isSigner: false, isWritable: true },
    ],
    programId: PROGRAM_ID,
    data: Buffer.concat([Buffer.from([0x02]), encoderBytecode])
  }));
  await sendTransaction(encTx);

  // 4. TX2: Decoder
  const decTx = new Transaction();
  decTx.add(new TransactionInstruction({
    keys: [
      { pubkey: payer.publicKey, isSigner: true, isWritable: true },
      { pubkey: hiddenAccount, isSigner: false, isWritable: false },
    ],
    programId: PROGRAM_ID,
    data: Buffer.concat([Buffer.from([0x02]), decoderBytecode])
  }));
  const result = await sendTransaction(decTx);

  // 5. Interpret result
  const direction = result.returnData > 0 ? "BULLISH" :
                    result.returnData < 0 ? "BEARISH" : "NEUTRAL";

  return direction;
}</code></pre>

            <h3>8.2 Complete Example</h3>

            <pre><code>// Get current features from Pyth
const pythData = await getPythPrice();
const features = calculateFeatures(priceHistory);

// Normalize (0-255)
const normalized = [
  Math.floor(features.vwapRatio * 128),      // 0-255
  Math.floor(128 + features.volumeAccel * 256),
  Math.floor(128 + features.momentum * 1280),
  Math.floor(features.volatility * 25500)
];

// Predict
const prediction = await predict(normalized);
console.log(`Prediction: ${prediction}`);</code></pre>

            <h3>8.3 Interpreting Results</h3>

            <table>
                <tr>
                    <th>Output o0</th>
                    <th>Interpretation</th>
                    <th>Suggested Action</th>
                </tr>
                <tr>
                    <td>&gt; 10</td>
                    <td><span class="badge badge-green">BULLISH</span></td>
                    <td>Potential buy signal</td>
                </tr>
                <tr>
                    <td>-10 to 10</td>
                    <td><span class="badge badge-yellow">NEUTRAL</span></td>
                    <td>Wait / Hold</td>
                </tr>
                <tr>
                    <td>&lt; -10</td>
                    <td><span class="badge badge-red">BEARISH</span></td>
                    <td>Potential sell signal</td>
                </tr>
            </table>
        </section>

        <!-- LIMITATIONS -->
        <section id="limitations">
            <h2>9. Limitations and Warnings</h2>

            <div class="error">
                <strong>IMPORTANT WARNING</strong>
                <p>
                    This model is experimental and should NOT be used for actual investment decisions.
                    Financial markets are inherently unpredictable in the short term.
                </p>
            </div>

            <h3>9.1 Technical Limitations</h3>

            <table>
                <tr>
                    <th>Limitation</th>
                    <th>Impact</th>
                    <th>Cause</th>
                </tr>
                <tr>
                    <td>Accuracy ~45-50%</td>
                    <td>High</td>
                    <td>Efficient markets, random noise</td>
                </tr>
                <tr>
                    <td>Only 23 parameters</td>
                    <td>Medium</td>
                    <td>946 bytes bytecode limit</td>
                </tr>
                <tr>
                    <td>INT8 quantization</td>
                    <td>Low</td>
                    <td>Precision loss ~5%</td>
                </tr>
                <tr>
                    <td>2 transactions required</td>
                    <td>Medium</td>
                    <td>Solana TX size limit</td>
                </tr>
                <tr>
                    <td>Latency ~1-2s</td>
                    <td>Low</td>
                    <td>Solana TX confirmation</td>
                </tr>
            </table>

            <h3>9.2 Why Accuracy is Limited</h3>

            <p>Short-term price prediction is one of the hardest problems in ML:</p>

            <ol style="margin-left: 2rem; margin-bottom: 1rem;">
                <li><strong>Efficient Market Hypothesis:</strong> Prices already reflect all available information</li>
                <li><strong>Random walk:</strong> Short-term price movements are essentially random</li>
                <li><strong>Model too simple:</strong> 23 parameters cannot capture market complexity</li>
                <li><strong>Limited features:</strong> 4 inputs represent only a fraction of market information</li>
            </ol>

            <h3>9.3 Possible Improvements</h3>

            <table>
                <tr>
                    <th>Improvement</th>
                    <th>Potential Impact</th>
                    <th>Effort</th>
                </tr>
                <tr>
                    <td>More features (6-8)</td>
                    <td>+5-10% accuracy</td>
                    <td>Medium (3 TX)</td>
                </tr>
                <tr>
                    <td>Larger hidden layer (4-5)</td>
                    <td>+3-5% accuracy</td>
                    <td>Medium</td>
                </tr>
                <tr>
                    <td>Ensemble of models</td>
                    <td>+5-10% accuracy</td>
                    <td>High</td>
                </tr>
                <tr>
                    <td>Advanced features (order flow)</td>
                    <td>+10-15% accuracy</td>
                    <td>Very high</td>
                </tr>
            </table>

            <div class="warning">
                <strong>Reality of algorithmic trading:</strong>
                <p>
                    Professional quantitative funds use models with millions of parameters,
                    proprietary data (order flow, dark pools), and co-located infrastructure.
                    Even with these advantages, most only achieve 51-55% accuracy on short-term trades.
                </p>
            </div>
        </section>

        <!-- STRUCTURE -->
        <section id="structure">
            <h2>10. Project Structure</h2>

            <pre><code>price-predictor/
+-- src/
|   +-- python/
|   |   +-- price_4_3_s1_trained.py  # Encoder with weights
|   |   +-- price_4_3_s2_trained.py  # Decoder with weights
|   +-- scripts/
|       +-- train_v2.py              # Training pipeline
|       +-- collect_data.js          # Data collection
|       +-- test.js                  # Solana E2E tests
+-- data/
|   +-- sol_market_data.csv          # Dataset
+-- weights/
|   +-- encoder_4_3.bin              # Encoder weights (19 bytes)
|   +-- decoder_4_3.bin              # Decoder weights (8 bytes)
|   +-- weights_debug.txt            # Human-readable weights
+-- docs/
    +-- documentation.html           # This file</code></pre>
        </section>

        <footer>
            <p>Price Predictor On-Chain - Documentation v1.0</p>
            <p>Built with SolanaPython | <span class="badge badge-blue">Experimental</span></p>
        </footer>
    </div>
</body>
</html>
